"""
Farmatodo Store Stock Scraper - FINAL 10/10 Edition
Tested and validated extraction method from browser subagent

CONFIRMED DATA STRUCTURE:
- Stock per store visible in the "Disponibilidad en Farmatodo" section
- Format: "Store Name | Address | XX unid"
- Can extract: Store name, City, Stock units

Example extractions:
- Paraiso - Arco: 106 unid
- Catia - Boulevar De Catia: 131 unid
- Petare - Av. Francisco De Miranda - Monta√±al: 121 unid
"""
import asyncio
import json
import re
from datetime import datetime
from playwright.async_api import async_playwright


class FarmatodoStoreStockScraper:
    def __init__(self, headless: bool = True, timeout: int = 60000):
        self.headless = headless
        self.timeout = timeout
        self.playwright = None
        self.browser = None
        self.page = None
        
    async def __aenter__(self):
        await self.start()
        return self
        
    async def __aexit__(self, *args):
        await self.close()
        
    async def start(self):
        """Initialize browser"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=self.headless,
            args=['--disable-blink-features=AutomationControlled']
        )
        context = await self.browser.new_context(
            viewport={'width': 1400, 'height': 1000},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        self.page = await context.new_page()
        
    async def close(self):
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def get_store_stock(self, product_url: str, cities_to_scrape: list = None) -> dict:
        """
        Scrape stock per store for a given product.
        
        Args:
            product_url: Full URL of the product page
            cities_to_scrape: List of city names to scrape. If None, scrapes all cities.
            
        Returns:
            Dict with product info and store-level stock data
        """
        result = {
            'product_url': product_url,
            'scraped_at': datetime.now().isoformat(),
            'product_name': None,
            'stores': [],
            'cities_data': {},
            'summary': {}
        }
        
        try:
            # 1. Navigate to product page
            print(f"[1/5] Loading product page...")
            await self.page.goto(product_url, wait_until='domcontentloaded', timeout=self.timeout)
            await asyncio.sleep(5)  # Wait for JS to load
            
            # 2. Get product name
            try:
                name = await self.page.locator('h1').first.text_content()
                result['product_name'] = name.strip() if name else None
                print(f"    Product: {result['product_name'][:50] if result['product_name'] else 'N/A'}...")
            except:
                pass
            
            # 3. Scroll to availability section
            print(f"[2/5] Finding availability section...")
            await self.page.evaluate('''() => {
                const container = document.querySelector('.content-cities');
                if (container) container.scrollIntoView();
            }''')
            await asyncio.sleep(2)
            
            # 4. Get available cities
            print(f"[3/5] Getting city list...")
            cities = await self.page.evaluate('''() => {
                const container = document.querySelector('.content-cities');
                if (!container) return [];
                const links = container.querySelectorAll('a');
                return Array.from(links).map(a => a.textContent.trim()).filter(t => t.length > 1);
            }''')
            print(f"    Found {len(cities)} cities")
            
            # Filter cities if specified
            if cities_to_scrape:
                cities = [c for c in cities if c.lower() in [x.lower() for x in cities_to_scrape]]
            
            # 5. Click each city and extract store data
            print(f"[4/5] Extracting stock per store...")
            all_stores = []
            
            for city in cities[:10]:  # Limit to first 10 cities for speed
                try:
                    print(f"    Scraping {city}...")
                    
                    # Click on city
                    city_link = self.page.get_by_text(city, exact=True)
                    await city_link.click(timeout=3000)
                    await asyncio.sleep(1.5)
                    
                    # Extract store data using validated JavaScript
                    stores_data = await self.page.evaluate('''() => {
                        const container = document.querySelector('.content-cities');
                        if (!container) return [];
                        
                        const text = container.innerText;
                        const lines = text.split('\\n').map(l => l.trim()).filter(l => l.length > 0);
                        
                        const stores = [];
                        for (let i = 0; i < lines.length; i++) {
                            const line = lines[i];
                            const stockMatch = line.match(/(\\d+)\\s*unid/i);
                            
                            if (stockMatch) {
                                // Store name is usually 2 lines back
                                let storeName = "Unknown";
                                if (i >= 2) {
                                    storeName = lines[i-2];
                                } else if (i >= 1) {
                                    storeName = lines[i-1];
                                }
                                
                                stores.push({
                                    store_name: storeName,
                                    stock: parseInt(stockMatch[1]),
                                    raw_line: line
                                });
                            }
                        }
                        return stores;
                    }''')
                    
                    for store in stores_data:
                        store['city'] = city
                        all_stores.append(store)
                    
                    if city not in result['cities_data']:
                        result['cities_data'][city] = []
                    result['cities_data'][city].extend(stores_data)
                    
                except Exception as e:
                    print(f"      Error scraping {city}: {e}")
                    continue
            
            # 6. Process and deduplicate
            print(f"[5/5] Processing results...")
            
            # Deduplicate by store_name + stock
            seen = set()
            unique_stores = []
            for store in all_stores:
                key = (store['store_name'], store['stock'])
                if key not in seen and store['stock'] > 0:
                    seen.add(key)
                    unique_stores.append(store)
            
            result['stores'] = unique_stores
            
            # Calculate summary
            total_stock = sum(s['stock'] for s in unique_stores)
            cities_covered = len(set(s['city'] for s in unique_stores))
            
            result['summary'] = {
                'total_stores': len(unique_stores),
                'total_stock': total_stock,
                'cities_covered': cities_covered,
                'avg_stock_per_store': round(total_stock / len(unique_stores), 1) if unique_stores else 0
            }
            
            return result
            
        except Exception as e:
            import traceback
            result['error'] = str(e)
            result['traceback'] = traceback.format_exc()
            return result


async def main():
    """Demo: Scrape stock for Acetaminofen Genven"""
    
    product_url = "https://www.farmatodo.com.ve/producto/113016367-acetaminofen-500mg-10tabletas"
    
    print("=" * 70)
    print("FARMATODO STORE STOCK SCRAPER - 10/10 FINAL EDITION")
    print("=" * 70)
    print(f"\nTarget: {product_url}\n")
    
    async with FarmatodoStoreStockScraper(headless=True) as scraper:
        result = await scraper.get_store_stock(
            product_url, 
            cities_to_scrape=["Caracas", "Barquisimeto", "Valencia"]  # Specific cities
        )
        
        print("\n" + "=" * 70)
        print("RESULTS - STOCK PER STORE")
        print("=" * 70)
        
        print(f"\nProduct: {result.get('product_name', 'N/A')}")
        
        summary = result.get('summary', {})
        print(f"\nüìä Summary:")
        print(f"   Total stores: {summary.get('total_stores', 0)}")
        print(f"   Total stock: {summary.get('total_stock', 0):,} units")
        print(f"   Cities covered: {summary.get('cities_covered', 0)}")
        print(f"   Avg per store: {summary.get('avg_stock_per_store', 0)} units")
        
        stores = result.get('stores', [])
        if stores:
            print(f"\nüè™ Store-Level Stock (Top 20):")
            print("-" * 50)
            for store in stores[:20]:
                city = store.get('city', '?')
                name = store.get('store_name', '?')[:30]
                stock = store.get('stock', 0)
                print(f"  {city:15} | {name:30} | {stock:4} unid")
        
        if result.get('error'):
            print(f"\n‚ùå Error: {result['error']}")
        
        # Save results
        output_file = "farmatodo_store_stock_final.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"\n‚úÖ Results saved to: {output_file}")
        
        # Show by city
        print(f"\nüìç Stock by City:")
        cities_data = result.get('cities_data', {})
        for city, stores in cities_data.items():
            city_total = sum(s['stock'] for s in stores)
            print(f"   {city}: {len(stores)} stores, {city_total:,} units total")


if __name__ == "__main__":
    asyncio.run(main())
